%*****************************************
\chapter{Introduction}
%*****************************************
The thesis on hand discusses the possible extensions of a specific neural network pruning technique presented by J. Frankle and Michael at the beginning of 2019.\cite{LTH}\\
The next few paragraphs motivate the research,  specify its extent, and give an outline for the remaining chapters.

\section{Motivation}
Over the last decade, neural networks have become ever more prevalent in applications of any kind. They can theoretically approximate most mappings between inputs and expected output\footnote{The class of networks with at least one hidden layer can approximate any borel-measurable function. Most real-world mappings are implicitly assumed to be measurable functions as long as they deterministically map one specific input to a single specific output.
} up to arbitrary precision\footnote{The network in question must be allowed arbitrary size to achieve an approximation of arbitrary precision.
}\cite{Approximator}, and in practice, they achieve results unobtainable with previous technologies.\\ 
As computational resources became more available, state-of-the-art approaches featured ever-larger networks. Although they excel at their tasks, it is challenging to execute them on small portable devices such as smartphones. Pruning techniques aim to reduce the size architectures have at runtime through the removal of parts that are estimated to no longer be necessary after training. Still, there is no consensus on how to identify these superfluous sections.\\
In their paper "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks," J. Frankle and M. Carbin demonstrate an innovative algorithm to remove up to 90 percent of various networks while retaining most or all of their performance. These results are comparable to other pruning techniques while promising new insights into the nature of neural networks.\cite{LTH}
\\
The following chapters of this work explore the transfer of the Lottery Ticket Hypothesis to another field as well as earlier retrieval of lottery tickets on a previously studied architecture.

\section{Problem Statement and Contribution}
In the latter half of their paper, J. Frankle and M. Carbin also discuss the limitations of their work. First and foremost, they applied their method only to small datasets because iterative pruning is computationally expensive.\cite{LTH} Both datasets they studied are from the field of image recognition, and it is uncertain whether their results are transferable to other contexts such as natural language processing or voice recognition. This thesis aims to provide a proof-of-concept for the application of their algorithm on the task of document topic classification in the field of natural language processing.\\
Furthermore, J. Frankle and M. Carbin acknowledge that pruning single connections instead of whole sections of a network does not line up well with modern frameworks, and thus, no actual speed-up is achieved. Finally, they state that they plan to research ways to find lottery tickets earlier or at smaller sizes.\cite{LTH} The third experiment of this work explores the possibility of finding small subnetworks earlier into training, which would speed up the pruning procedure.


\section{Outline}

After this introduction, chapter \ref{ch:background} of this thesis establishes the necessary background for the upcoming descriptions and discussions. Afterward, section \ref{ch:relatedwork} gives an overview of the work already done on the Lottery Ticket Hypothesis and the tasks absolved during the experiments. Chapter \ref{ch:design} describes the design of the experiments without mention of the implementation details, which follow in section \ref{ch:implementation}. Before the discussion of the results, chapter \ref{ch:datasets} supplies additional information on the datasets utilized in this thesis.\\
Finally, section \ref{ch:evaluation} presents an evaluation, and chapter \ref{ch:closure} concludes this work with a summary and a mention of possibilities for future work.