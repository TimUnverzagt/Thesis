\begin{abstract}
Neural network applications of ever-growing size and their adaptations for a growing number of different and sometimes less powerful devices necessitate pruning, the reduction in network size through the removal of superfluous substructures. The term pruning usually implies that the network in question has absolved training until convergence; techniques for a-priori reductions are generally referred to as network architecture search.\\
While most pruning techniques retain the weights of the trained network, in their paper "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks," J. Frankle and M. Carbin present a novel technique which resets the weights of the pruned network to their pre-training values.\\
\\
The primary aim of this thesis is to extend their work and research whether the presented algorithm can provide competitive pruning on different datasets, specifically in the field of natural language processing. As the reset to initial values leaves said algorithm similar to a network architecture search method, the pruning can be interpreted as a search dependant on the weights of the trained network. This work also studies how this search develops in quality over the amount of training provided beforehand.\\
\\
In pursuit of these two goals, we developed a framework through which we implemented four experiments. The first and second experiments aim to confirm the validity of the codebase. Subsequently, the third one intents to establish an argument for the usage of the pruning method in the field of natural language processing. With the last experiment, we plan to provide explorative data to inform further study on the emergence of actionable experience in the values of a network.\\
\\
While the first two experiments fail to reproduce the results of J. Frankle and M. Carbin, one of them prunes its associated networks up to the scale contemporary technique, namely a size reduction of 90 percent with little loss in accuracy. Additionally, the third result conforms with their definition of a lottery ticket, achieving the same or even better quality measure, up to the same degree of compression. Finally, the data of the exploratory experiment show low legibility due to the erratic results of single prunings. Nonetheless, none of the early pruning trials achieve the same level of accuracy displayed by our previous experiment, even if the latter trials prune as late as epoch 10, an amount of training for which the full network already converges. These results suggest that, during the pruning iterations,  the pruning algorithm has to submit a given network to more training than necessary for primary convergence. 
\end{abstract}




%*****************************************
%*************Introduction****************
%*****************************************
\include{introduction}

%*****************************************
%**************Background*****************
%*****************************************
\include{background}

%*****************************************
%*************Related Work****************
%*****************************************
\include{related_work}

%*****************************************
%****************Design*******************
%*****************************************
\include{design}

%*****************************************
%************Implementation***************
%*****************************************
\include{implementation}

%*****************************************
%**************Data Sets******************
%*****************************************
\include{data_sets}

%*****************************************
%**************Evaluation*****************
%*****************************************
\include{evaluation}


%*****************************************
%**************Conclusions****************
%*****************************************
\include{conclusions}
