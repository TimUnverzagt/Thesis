\begin{abstract}
Neural network applications of ever-growing size and their adaptations for a growing number of different and sometimes less powerful devices necessitate pruning; The reduction in network size through the removal of superfluous substructures. The term pruning usually implies that the network in question has absolved training until convergence; techniques for a-priori reductions are generally referred to as network architecture search.\\
While most pruning techniques retain the weights of the trained network, in their paper "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks," J. Frankle and M. Carbin present a novel technique which resets the weights of the pruned network to their pre-training values.\\
\\
The primary aim of this thesis is to extend their work and research whether the presented algorithm can provide competitive pruning on different datasets, specifically in the field of natural language processing. As the reset to initial values leaves said algorithm similar to a network architecture search method, the pruning can be interpreted as a search dependant on the weights of the trained network. This work also studies how this search develops in quality over the amount of training provided beforehand.\\
\\
In pursuit of these two goals, we developed a framework through in we implemented four experiments. The first two experiments aimed to confirm the validity of the codebase. Subsequently, the third one intended to establish an argument for application of their pruning method in the field of natural language processing. With the last experiment, we plan to provide explorative data to inform further study on the emergence of actionable experience in the values of a network.\\
\\
While the first two experiments failed to reproduce the results of J. Frankle and M. Carbin, one of them pruned its associated network up to the scale of contemporary technique. It achieved a size reduction of 90 percent with little loss in accuracy. Additionally, the third result conforms with their definition of a lottery ticket, producing the same or even better quality measure, up to the same degree of compression. Finally, the data of the exploratory experiment show low legibility due to the erratic results of single prunings. Nonetheless, none of the early pruning trials achieve the same level of accuracy displayed by our previous experiment, even if the latter trials pruned as late as epoch 10, an amount of training for which the full network already converged. These results suggest that, during the pruning iterations,  the pruning algorithm has to submit a given network to more training than necessary for primary convergence. 
\end{abstract}




%*****************************************
%*************Introduction****************
%*****************************************
\include{introduction}

%*****************************************
%**************Background*****************
%*****************************************
\include{background}

%*****************************************
%*************Related Work****************
%*****************************************
\include{related_work}

%*****************************************
%****************Design*******************
%*****************************************
\include{design}

%*****************************************
%************Implementation***************
%*****************************************
\include{implementation}

%*****************************************
%**************Data Sets******************
%*****************************************
\include{data_sets}

%*****************************************
%**************Evaluation*****************
%*****************************************
\include{evaluation}


%*****************************************
%**************Conclusions****************
%*****************************************
\include{conclusions}
