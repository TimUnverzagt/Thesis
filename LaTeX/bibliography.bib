% Encoding: windows-1252

@InCollection{Learning_Weights_And_Connections,
  author        = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  title         = {Learning both Weights and Connections for Efficient Neural Network},
  booktitle     = {Advances in Neural Information Processing Systems 28},
  publisher     = {Curran Associates, Inc.},
  year          = {2015},
  editor        = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages         = {1135--1143},
  __markedentry = {[TIm:6]},
  url           = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
}
{He_2015_ICCV,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},

@Article{Deconstructing_LTH,
  author        = {Hattie Zhou and Janice Lan and Rosanne Liu and Jason Yosinski},
  title         = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1905.01067},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-01067},
  eprint        = {1905.01067},
  timestamp     = {Mon, 27 May 2019 13:15:00 +0200},
  url           = {http://arxiv.org/abs/1905.01067},
}

@Article{LTH-At-Scale,
  author        = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  title         = {The Lottery Ticket Hypothesis at Scale},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1903.01611},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1903-01611},
  eprint        = {1903.01611},
  timestamp     = {Sat, 30 Mar 2019 19:27:21 +0100},
  url           = {http://arxiv.org/abs/1903.01611},
}

@Article{LTH,
  author        = {Jonathan Frankle and Michael Carbin},
  title         = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1803.03635},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1803-03635},
  eprint        = {1803.03635},
  timestamp     = {Mon, 13 Aug 2018 16:48:29 +0200},
  url           = {http://arxiv.org/abs/1803.03635},
}

@Article{Morcos2019,
  author  = {Ari S. Morcos and Haonan Yu and Michela Paganini and Yuandong Tian},
  title   = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1906.02773},
}

@Article{Mehta2019,
  author  = {Rashi I. Mehta},
  title   = {Sparse Transfer Learning via Winning Lottery Tickets},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1905.07785},
}

@Article{Elsken2018,
  author  = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title   = {Neural Architecture Search: A Survey},
  journal = {J. Mach. Learn. Res.},
  year    = {2018},
  volume  = {20},
  pages   = {55:1-55:21},
}

@InProceedings{Soelen2019,
  author = {Ryan Van Soelen},
  title  = {Using Winning Lottery Tickets in Transfer Learning for Convolutional Neural Networks},
  year   = {2019},
}

@Article{Ghiassi2012a,
  author   = {M. Ghiassi and M. Olschimke and B. Moon and P. Arnaudo},
  title    = {Automated text classification using a dynamic artificial neural network model},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {12},
  pages    = {10967 - 10976},
  issn     = {0957-4174},
  abstract = {Widespread digitization of information in today�s internet age has intensified the need for effective textual document classification algorithms. Most real life classification problems, including text classification, genetic classification, medical classification, and others, are complex in nature and are characterized by high dimensionality. Current solution strategies include Na�ve Bayes (NB), Neural Network (NN), Linear Least Squares Fit (LLSF), k-Nearest-Neighbor (kNN), and Support Vector Machines (SVM); with SVMs showing better results in most cases. In this paper we introduce a new approach called dynamic architecture for artificial neural networks (DAN2) as an alternative for solving textual document classification problems. DAN2 is a scalable algorithm that does not require parameter settings or network architecture configuration. To show DAN2 as an effective and scalable alternative for text classification, we present comparative results for the Reuters-21578 benchmark dataset. Our results show DAN2 to perform very well against the current leading solutions (kNN and SVM) using established classification metrics.},
  doi      = {https://doi.org/10.1016/j.eswa.2012.03.027},
  keywords = {Classification, Textual document classification, Dynamic artificial neural networks, Pattern recognition, Machine learning, Artificial intelligence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412004976},
}

@InProceedings{Nam2014a,
  author    = {Nam, Jinseok and Kim, Jungi and Loza Menc{\'i}a, Eneldo and Gurevych, Iryna and F{\"u}rnkranz, Johannes},
  title     = {Large-Scale Multi-label Text Classification --- Revisiting Neural Networks},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  year      = {2014},
  editor    = {Calders, Toon and Esposito, Floriana and H{\"u}llermeier, Eyke and Meo, Rosa},
  pages     = {437--452},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn      = {978-3-662-44851-9},
}

@Article{Reuters-Subsets,
  author   = {Debole, Franca and Sebastiani, Fabrizio},
  title    = {An analysis of the relative hardness of Reuters-21578 subsets},
  journal  = {Journal of the American Society for Information Science and Technology},
  year     = {2005},
  volume   = {56},
  number   = {6},
  pages    = {584-596},
  abstract = {Abstract The existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (IR) task are beneficial to research on this task, because they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark. The Reuters-21578 test collection, together with its earlier variants, has been such a standard benchmark for the text categorization (TC) task throughout the last 10 years. However, the benefits that this has brought about have somehow been limited by the fact that different researchers have �carved� different subsets out of this collection and tested their systems on one of these subsets only; systems that have been tested on different Reuters-21578 subsets are thus not readily comparable. In this article, we present a systematic, comparative experimental study of the three subsets of Reuters-21578 that have been most popular among TC researchers. The results we obtain allow us to determine the relative hardness of these subsets, thus establishing an indirect means for comparing TC systems that have, or will be, tested on these different subsets.},
  doi      = {10.1002/asi.20147},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20147},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20147},
}

@InProceedings{CapdevilaDalmau2007a,
  author    = {Capdevila Dalmau, Marta and M{\'a}rquez Fl{\'o}rez, Oscar W.},
  title     = {Experimental Results of the Signal Processing Approach to Distributional Clustering of Terms on Reuters-21578 Collection},
  booktitle = {Advances in Information Retrieval},
  year      = {2007},
  editor    = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
  pages     = {678--681},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Distributional Clustering has showed to be an effective and powerful approach to supervised term extraction aimed at reducing the original indexing space dimensionality for Automatic Text Categorization [2]. In a recent paper [1] we introduced a new Signal Processing approach to Distributional Clustering which reached categorization results on 20 Newsgroups dataset similar to those obtained by other information-theoretic approaches [3][4][5] . Here we re-validate our method by showing that the 90-categories Reuters-21578 benchmark collection can be indexed with a minimum loss of categorization accuracy (around 2{\%} with Na{\"i}ve Bayes categorizer) with only 50 clusters.},
  isbn      = {978-3-540-71496-5},
}

@InProceedings{Rose2002,
  author       = {Rose, Tony and Stevenson, Mark and Whitehead, Miles},
  title        = {The Reuters Corpus Volume 1-from Yesterday's News to Tomorrow's Language Resources.},
  booktitle    = {Lrec},
  year         = {2002},
  volume       = {2},
  pages        = {827--832},
  organization = {Las Palmas},
}

@InProceedings{Goyal2007,
  author    = {R. D. {Goyal}},
  title     = {Knowledge Based Neural Network for Text Classification},
  booktitle = {2007 IEEE International Conference on Granular Computing (GRC 2007)},
  year      = {2007},
  pages     = {542-542},
  month     = {Nov},
  doi       = {10.1109/GrC.2007.108},
  keywords  = {Bayes methods;neural nets;text analysis;knowledge based neural network;automatic text classification;Bayesian method;training documents;noise data;naive Bayesian text classification technique;Neural networks;Text categorization;Training data;Bayesian methods;Machine learning algorithms;Computer networks;Information technology;Learning systems;Data analysis;Probability distribution},
}

@InProceedings{Chen2017,
  author    = {G. {Chen} and D. {Ye} and Z. {Xing} and J. {Chen} and E. {Cambria}},
  title     = {Ensemble application of convolutional and recurrent neural networks for multi-label text categorization},
  booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
  year      = {2017},
  pages     = {2377-2383},
  month     = {May},
  doi       = {10.1109/IJCNN.2017.7966144},
  issn      = {2161-4407},
  keywords  = {computational complexity;feature extraction;feedforward neural nets;pattern classification;recurrent neural nets;text analysis;ensemble application;convolutional neural networks;recurrent neural networks;multilabel text categorization;text classification;document semantic information;multiclass text categorization;label combinations;local semantic information extraction;global textual semantics;local textual semantics;high-order label correlation modelling;tractable computational complexity;CNN-RNN model;large-sized dataset;Text categorization;Feature extraction;Semantics;Correlation;Logic gates;Predictive models;Computational modeling},
}

@Article{Violos2018,
  author  = {Violos, John and Tserpes, Konstantinos and Varlamis, Iraklis and Varvarigou, Theodora},
  title   = {Text Classification using the N-gram graph Representation model over high frequency data streams},
  journal = {Front. Appl. Math. Stat. 4: 41. doi: 10.3389/fams},
  year    = {2018},
}

@Article{Miao2009a,
  author        = {Duoqian Miao and Qiguo Duan and Hongyun Zhang and Na Jiao},
  title         = {Rough set based hybrid algorithm for text classification},
  journal       = {Expert Systems with Applications},
  year          = {2009},
  volume        = {36},
  number        = {5},
  pages         = {9168 - 9174},
  issn          = {0957-4174},
  __markedentry = {[TIm:]},
  abstract      = {Automatic classification of text documents, one of essential techniques for Web mining, has always been a hot topic due to the explosive growth of digital documents available on-line. In text classification community, k-nearest neighbor (kNN) is a simple and yet effective classifier. However, as being a lazy learning method without premodelling, kNN has a high cost to classify new documents when training set is large. Rocchio algorithm is another well-known and widely used technique for text classification. One drawback of the Rocchio classifier is that it restricts the hypothesis space to the set of linear separable hyperplane regions. When the data does not fit its underlying assumption well, Rocchio classifier suffers. In this paper, a hybrid algorithm based on variable precision rough set is proposed to combine the strength of both kNN and Rocchio techniques and overcome their weaknesses. An experimental evaluation of different methods is carried out on two common text corpora, i.e., the Reuters-21578 collection and the 20-newsgroup collection. The experimental results indicate that the novel algorithm achieves significant performance improvement.},
  doi           = {https://doi.org/10.1016/j.eswa.2008.12.026},
  keywords      = {Text classification, Variable precision rough set (VPRS), -nearest neighbor (kNN), Rocchio algorithm},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417408008919},
}

@Article{CNN-For-NLP,
  author        = {Yoon Kim},
  title         = {Convolutional Neural Networks for Sentence Classification},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1408.5882},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/Kim14f},
  eprint        = {1408.5882},
  timestamp     = {Mon, 13 Aug 2018 16:46:21 +0200},
  url           = {http://arxiv.org/abs/1408.5882},
}

@Article{Pruning-With-Little-Training,
  author        = {Yue Li and Weibin Zhao and Lin Shang},
  title         = {Really should we pruning after model be totally trained? Pruning based on a small amount of training},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1901.08455},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-08455},
  eprint        = {1901.08455},
  timestamp     = {Sat, 02 Feb 2019 16:56:00 +0100},
  url           = {http://arxiv.org/abs/1901.08455},
}

@Article{Rozental2018,
  author        = {Alon Rozental and Daniel Fleischer and Zohar Kelrich},
  title         = {Amobee at {IEST} 2018: Transfer Learning from Language Models},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1808.08782},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1808-08782},
  eprint        = {1808.08782},
  timestamp     = {Sun, 02 Sep 2018 15:01:54 +0200},
  url           = {http://arxiv.org/abs/1808.08782},
}

@Article{Wortsman2019,
  author  = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  title   = {Discovering Neural Wirings},
  journal = {arXiv preprint arXiv:1906.00586},
  year    = {2019},
}

@Book{Adamy2007,
  title     = {Fuzzy Logik, Neuronale Netze und Evolution{\"a}re Algorithmen},
  publisher = {Shaker},
  year      = {2007},
  author    = {Adamy, J{\"u}rgen},
}

@Article{Poggio2001,
  author  = {Poggio, Tomaso and Girosi, Federico},
  title   = {A Theory of Networks for Approximation and Learning},
  journal = {Tech Rep A.I. Memo No. 1140},
  year    = {2001},
  volume  = {1140},
  month   = {08},
}

@InProceedings{He_2015_ICCV,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  month     = {December},
}

@Article{Russakovsky2015,
  author   = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title    = {ImageNet Large Scale Visual Recognition Challenge},
  journal  = {International Journal of Computer Vision},
  year     = {2015},
  volume   = {115},
  number   = {3},
  pages    = {211--252},
  month    = {Dec},
  issn     = {1573-1405},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5�years of the challenge, and propose future directions and improvements.},
  day      = {01},
  doi      = {10.1007/s11263-015-0816-y},
  url      = {https://doi.org/10.1007/s11263-015-0816-y},
}

@Article{lally2011natural,
  author  = {Lally, Adam and Fodor, Paul},
  title   = {Natural language processing with prolog in the ibm watson system},
  journal = {The Association for Logic Programming (ALP) Newsletter},
  year    = {2011},
}

@Article{silver2017mastering,
  author    = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  title     = {Mastering the game of go without human knowledge},
  journal   = {Nature},
  year      = {2017},
  volume    = {550},
  number    = {7676},
  pages     = {354},
  publisher = {Nature Publishing Group},
}

@Article{gibney2016google,
  author  = {Gibney, Elizabeth},
  title   = {Google AI algorithm masters ancient game of Go},
  journal = {Nature News},
  year    = {2016},
  volume  = {529},
  number  = {7587},
  pages   = {445},
}

@Article{ARON201124,
  author  = {Jacob Aron},
  title   = {How innovative is Apple's new voice assistant, Siri?},
  journal = {New Scientist},
  year    = {2011},
  volume  = {212},
  number  = {2836},
  pages   = {24},
  issn    = {0262-4079},
  doi     = {https://doi.org/10.1016/S0262-4079(11)62647-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S026240791162647X},
}

@InProceedings{ThiNet,
  author    = {Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  title     = {ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2017},
  month     = {Oct},
}

@InCollection{Overparametrization,
  author    = {Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc\textquotesingle Aurelio and de Freitas, Nando},
  title     = {Predicting Parameters in Deep Learning},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  year      = {2013},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {2148--2156},
  url       = {http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf},
}

@InCollection{NIPS2014_5544,
  author    = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  title     = {Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation},
  booktitle = {Advances in Neural Information Processing Systems 27},
  publisher = {Curran Associates, Inc.},
  year      = {2014},
  editor    = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
  pages     = {1269--1277},
  url       = {http://papers.nips.cc/paper/5544-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.pdf},
}

@Article{Moor2006,
  author       = {Moor, James},
  title        = {The Dartmouth College Artificial Intelligence Conference: The Next Fifty Years},
  journal      = {AI Magazine},
  year         = {2006},
  volume       = {27},
  number       = {4},
  pages        = {87},
  month        = {Dec.},
  abstractnote = {The Dartmouth College Artificial Intelligence Conference: The Next 50 Years (AI@50) took place July 13-15, 2006. The conference had three objectives: to celebrate the Dartmouth Summer Research Project, which occurred in 1956; to assess how far AI has progressed; and to project where AI is going or should be going. AI@50 was generously funded by the office of the Dean of Faculty and the office of the Provost at Dartmouth College, by DARPA, and by some private donors.},
  doi          = {10.1609/aimag.v27i4.1911},
  url          = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1911},
}

@Article{Du2018a,
  author      = {Simon S. Du and Jason D. Lee and Haochuan Li and Liwei Wang and Xiyu Zhai},
  title       = {Gradient Descent Finds Global Minima of Deep Neural Networks},
  abstract    = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in polynomial time for a deep over-parameterized neural network with residual connections (ResNet). Our analysis relies on the particular structure of the Gram matrix induced by the neural network architecture. This structure allows us to show the Gram matrix is stable throughout the training process and this stability implies the global optimality of the gradient descent algorithm. We further extend our analysis to deep residual convolutional neural networks and obtain a similar convergence result.},
  date        = {2018-11-09},
  eprint      = {http://arxiv.org/abs/1811.03804v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1811.03804v4:PDF},
  keywords    = {cs.LG, cs.AI, cs.CV, math.OC, stat.ML},
}

@InProceedings{Neyshabur2019,
  author    = {Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
  title     = {The role of over-parametrization in generalization of neural networks},
  booktitle = {International Conference on Learning Representations},
  year      = {2019},
  url       = {https://openreview.net/forum?id=BygfghAcYX},
}

@InProceedings{Liu-2017-ICCV,
  author    = {Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  title     = {Learning Efficient Convolutional Networks Through Network Slimming},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2017},
  month     = {Oct},
}

@InProceedings{Rethinking-LSTM,
  author    = {Adhikari, Ashutosh and Ram, Achyudh and Tang, Raphael and Lin, Jimmy},
  title     = {Rethinking complex neural network architectures for document classification},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  year      = {2019},
  pages     = {4046--4051},
}

@InProceedings{RMDL,
  author    = {Kowsari, Kamran and Heidarysafa, Mojtaba and Brown, Donald E. and Meimandi, Kiana Jafari and Barnes, Laura E.},
  title     = {RMDL: Random Multimodel Deep Learning for Classification},
  booktitle = {Proceedings of the 2Nd International Conference on Information System and Data Mining},
  year      = {2018},
  series    = {ICISDM '18},
  pages     = {19--28},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3206111},
  doi       = {10.1145/3206098.3206111},
  isbn      = {978-1-4503-6354-9},
  keywords  = {Data Mining, Deep Learning, Deep Neural Networks, Image Classification, Supervised Learning, Text Classification},
  location  = {Lakeland, FL, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/3206098.3206111},
}

@InProceedings{yang-etal-2016-hierarchical,
  author    = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  title     = {Hierarchical Attention Networks for Document Classification},
  booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year      = {2016},
  pages     = {1480--1489},
  address   = {San Diego, California},
  month     = jun,
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/N16-1174},
  url       = {https://www.aclweb.org/anthology/N16-1174},
}

@Article{DBLP:journals/corr/Johnson014,
  author        = {Rie Johnson and Tong Zhang},
  title         = {Effective Use of Word Order for Text Categorization with Convolutional Neural Networks},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1412.1058},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/Johnson014},
  eprint        = {1412.1058},
  timestamp     = {Mon, 13 Aug 2018 16:48:27 +0200},
  url           = {http://arxiv.org/abs/1412.1058},
}

@Article{ReNet,
  author        = {Francesco Visin and Kyle Kastner and Kyunghyun Cho and Matteo Matteucci and Aaron C. Courville and Yoshua Bengio},
  title         = {ReNet: {A} Recurrent Neural Network Based Alternative to Convolutional Networks},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1505.00393},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/VisinKCMCB15},
  eprint        = {1505.00393},
  timestamp     = {Mon, 13 Aug 2018 16:46:40 +0200},
  url           = {http://arxiv.org/abs/1505.00393},
}

@Misc{MNIST,
  author       = {Yann LeCun, Corinna Cortes, Christopher J.C.Burges},
  title        = {THE MNIST DATABASE},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  timestamp    = {2019-12-02},
  url          = {http://yann.lecun.com/exdb/mnist/},
}

@Article{Multi-Column,
  author        = {Dan C. Ciresan and Ueli Meier and J{\"{u}}rgen Schmidhuber},
  title         = {Multi-column Deep Neural Networks for Image Classification},
  journal       = {CoRR},
  year          = {2012},
  volume        = {abs/1202.2745},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1202-2745},
  eprint        = {1202.2745},
  timestamp     = {Mon, 13 Aug 2018 16:47:05 +0200},
  url           = {http://arxiv.org/abs/1202.2745},
}

@Article{APAC,
  author        = {Ikuro Sato and Hiroki Nishimura and Kensuke Yokoi},
  title         = {{APAC:} Augmented PAttern Classification with Neural Networks},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1505.03229},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/SatoNY15},
  eprint        = {1505.03229},
  timestamp     = {Mon, 13 Aug 2018 16:47:34 +0200},
  url           = {http://arxiv.org/abs/1505.03229},
}

@Article{Batch-Normalized,
  author        = {Jia{-}Ren Chang and Yong{-}Sheng Chen},
  title         = {Batch-normalized Maxout Network in Network},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1511.02583},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ChangC15},
  eprint        = {1511.02583},
  timestamp     = {Mon, 13 Aug 2018 16:47:52 +0200},
  url           = {http://arxiv.org/abs/1511.02583},
}

@Article{Keep-It-Simple,
  author        = {Seyyed Hossein HasanPour and Mohammad Rouhani and Mohsen Fayyaz and Mohammad Sabokrou},
  title         = {Lets keep it simple, Using simple architectures to outperform deeper and more complex architectures},
  journal       = {CoRR},
  year          = {2016},
  volume        = {abs/1608.06037},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/HasanPourRVS16},
  eprint        = {1608.06037},
  timestamp     = {Mon, 13 Aug 2018 16:48:25 +0200},
  url           = {http://arxiv.org/abs/1608.06037},
}

@Article{Sparse-From-Sratch,
  author        = {Tim Dettmers and Luke Zettlemoyer},
  title         = {Sparse Networks from Scratch: Faster Training without Losing Performance},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1907.04840},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1907-04840},
  eprint        = {1907.04840},
  timestamp     = {Wed, 17 Jul 2019 10:27:35 +0200},
  url           = {http://arxiv.org/abs/1907.04840},
}

@InProceedings{Optimal-Brain-Damage,
  author    = {LeCun, Yann and Denker, John S and Solla, Sara A},
  title     = {Optimal brain damage},
  booktitle = {Advances in neural information processing systems},
  year      = {1990},
  pages     = {598--605},
}

@InProceedings{Skeletonization,
  author    = {Mozer, Michael C and Smolensky, Paul},
  title     = {Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  booktitle = {Advances in neural information processing systems},
  year      = {1989},
  pages     = {107--115},
}

@InProceedings{Optimal-Brain-Surgeon,
  author    = {Hassibi, Babak and Stork, David G},
  title     = {Second order derivatives for network pruning: Optimal brain surgeon},
  booktitle = {Advances in neural information processing systems},
  year      = {1993},
  pages     = {164--171},
}

@Misc{Reuters-21578,
  author       = {David D. Lewis},
  title        = {Reuters-21578},
  howpublished = {http://www.daviddlewis.com/resources/testcollections/reuters21578/},
  timestamp    = {2019-12-02},
  url          = {http://www.daviddlewis.com/resources/testcollections/reuters21578/},
}

@Article{Rethinking-Network-Pruning,
  author        = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  title         = {Rethinking the Value of Network Pruning},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1810.05270},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1810-05270},
  eprint        = {1810.05270},
  timestamp     = {Tue, 30 Oct 2018 20:39:56 +0100},
  url           = {http://arxiv.org/abs/1810.05270},
}

@Article{collobert2011natural,
  author  = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
  title   = {Natural language processing (almost) from scratch},
  journal = {Journal of machine learning research},
  year    = {2011},
  volume  = {12},
  number  = {Aug},
  pages   = {2493--2537},
}

@InProceedings{End-to-End-CNN,
  author    = {R. {Pappagari} and J. {Villalba} and N. {Dehak}},
  title     = {Joint Verification-Identification in end-to-end Multi-Scale CNN Framework for Topic Identification},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year      = {2018},
  pages     = {6199-6203},
  month     = {April},
  doi       = {10.1109/ICASSP.2018.8461673},
  issn      = {2379-190X},
  keywords  = {convolution;feedforward neural nets;learning (artificial intelligence);pattern classification;text analysis;word processing;end-to-end multiscale CNN framework;topic identification;topical word embeddings;parallel convolutional layers;joint verification-identification;end-to-end multiscale Convolutional Neural Network;classification;Task analysis;Training;Linear programming;Semantics;Support vector machines;Computational modeling;Convolution;BOW;raw text;CNN;verification;identification;topic id;end-to-end},
}

@Misc{20-Newsgroups,
  author       = {Jason Rennie},
  title        = {20-Newsgroups},
  howpublished = {http://qwone.com/~jason/20Newsgroups/},
  timestamp    = {2019-12-02},
  url          = {http://qwone.com/~jason/20Newsgroups/},
}

@Misc{EnAET,
  author        = {Xiao Wang and Daisuke Kihara and Jiebo Luo and Guo-Jun Qi},
  title         = {EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1911.09265},
  primaryclass  = {cs.CV},
}

@Article{Direct-NAS,
  author        = {Han Cai and Ligeng Zhu and Song Han},
  title         = {ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1812.00332},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1812-00332},
  eprint        = {1812.00332},
  timestamp     = {Tue, 01 Jan 2019 15:01:25 +0100},
  url           = {http://arxiv.org/abs/1812.00332},
}

@Article{Squee,
  author        = {Jie Hu and Li Shen and Gang Sun},
  title         = {Squeeze-and-Excitation Networks},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1709.01507},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1709-01507},
  eprint        = {1709.01507},
  timestamp     = {Mon, 13 Aug 2018 16:47:05 +0200},
  url           = {http://arxiv.org/abs/1709.01507},
}

@Misc{Neural-BoE,
  author        = {Ikuya Yamada and Hiroyuki Shindo},
  title         = {Neural Attentive Bag-of-Entities Model for Text Classification},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1909.01259},
  primaryclass  = {cs.CL},
}

@Misc{Graph-Star,
  author        = {Lu Haonan and Seth H. Huang and Tian Ye and Guo Xiuyan},
  title         = {Graph Star Net for Generalized Multi-Task Learning},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1906.12330},
  primaryclass  = {cs.SI},
}

@Article{universal,
  author    = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  title     = {Multilayer feedforward networks are universal approximators},
  journal   = {Neural networks},
  year      = {1989},
  volume    = {2},
  number    = {5},
  pages     = {359--366},
  publisher = {Elsevier},
}

@Article{lecun1998gradient,
  author    = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
  title     = {Gradient-based learning applied to document recognition},
  journal   = {Proceedings of the IEEE},
  year      = {1998},
  volume    = {86},
  number    = {11},
  pages     = {2278--2324},
  publisher = {Taipei, Taiwan},
}

@Article{CIFAR-Baseline,
  author        = {Tsung{-}Han Chan and Kui Jia and Shenghua Gao and Jiwen Lu and Zinan Zeng and Yi Ma},
  title         = {PCANet: {A} Simple Deep Learning Baseline for Image Classification?},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1404.3606},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ChanJGLZM14},
  eprint        = {1404.3606},
  timestamp     = {Tue, 25 Sep 2018 16:58:37 +0200},
  url           = {http://arxiv.org/abs/1404.3606},
}

@Book{,
}

@Book{Usenet,
  title     = {From Usenet to CoWebs},
  publisher = {Springer London},
  year      = {2002},
  author    = {Christopher Lueg and Danyel Fisher},
  isbn      = {1852335327},
  date      = {2002-11-12},
  ean       = {9781852335328},
  pagetotal = {276},
  url       = {https://www.ebook.de/de/product/3813952/from_usenet_to_cowebs.html},
}

@TechReport{CIFAR,
  author      = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  title       = {Learning multiple layers of features from tiny images},
  institution = {Citeseer},
  year        = {2009},
}

@Misc{CIFAR-Website,
  author       = {Alex Krizhevsky},
  title        = {The CIFAR-10 dataset},
  howpublished = {https://www.cs.toronto.edu/~kriz/cifar.html},
  timestamp    = {2020-01-12},
  url          = {https://www.cs.toronto.edu/~kriz/cifar.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
