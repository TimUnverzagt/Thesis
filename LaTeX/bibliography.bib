% Encoding: windows-1252

@Article{DBLP:journals/corr/abs-1905-01067,
  author        = {Hattie Zhou and Janice Lan and Rosanne Liu and Jason Yosinski},
  title         = {Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1905.01067},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1905-01067},
  eprint        = {1905.01067},
  timestamp     = {Mon, 27 May 2019 13:15:00 +0200},
  url           = {http://arxiv.org/abs/1905.01067},
}

@Article{DBLP:journals/corr/abs-1903-01611,
  author        = {Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  title         = {The Lottery Ticket Hypothesis at Scale},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1903.01611},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1903-01611},
  eprint        = {1903.01611},
  timestamp     = {Sat, 30 Mar 2019 19:27:21 +0100},
  url           = {http://arxiv.org/abs/1903.01611},
}

@Article{DBLP:journals/corr/abs-1803-03635,
  author        = {Jonathan Frankle and Michael Carbin},
  title         = {The Lottery Ticket Hypothesis: Training Pruned Neural Networks},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1803.03635},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1803-03635},
  eprint        = {1803.03635},
  timestamp     = {Mon, 13 Aug 2018 16:48:29 +0200},
  url           = {http://arxiv.org/abs/1803.03635},
}

@Article{Morcos2019OneTT,
  author  = {Ari S. Morcos and Haonan Yu and Michela Paganini and Yuandong Tian},
  title   = {One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1906.02773},
}

@Article{Liu2018RethinkingTV,
  author  = {Zhuang Liu and Mingjie Sun and Tinghui Zhou and Gao Huang and Trevor Darrell},
  title   = {Rethinking the Value of Network Pruning},
  journal = {ArXiv},
  year    = {2018},
  volume  = {abs/1810.05270},
}

@Article{Mehta2019SparseTL,
  author  = {Rashi I. Mehta},
  title   = {Sparse Transfer Learning via Winning Lottery Tickets},
  journal = {ArXiv},
  year    = {2019},
  volume  = {abs/1905.07785},
}

@Article{Elsken2018NeuralAS,
  author  = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
  title   = {Neural Architecture Search: A Survey},
  journal = {J. Mach. Learn. Res.},
  year    = {2018},
  volume  = {20},
  pages   = {55:1-55:21},
}

@InProceedings{Soelen2019UsingWL,
  author = {Ryan Van Soelen},
  title  = {Using Winning Lottery Tickets in Transfer Learning for Convolutional Neural Networks},
  year   = {2019},
}

@Article{Ghiassi2012,
  author   = {M. Ghiassi and M. Olschimke and B. Moon and P. Arnaudo},
  title    = {Automated text classification using a dynamic artificial neural network model},
  journal  = {Expert Systems with Applications},
  year     = {2012},
  volume   = {39},
  number   = {12},
  pages    = {10967 - 10976},
  issn     = {0957-4174},
  abstract = {Widespread digitization of information in today’s internet age has intensified the need for effective textual document classification algorithms. Most real life classification problems, including text classification, genetic classification, medical classification, and others, are complex in nature and are characterized by high dimensionality. Current solution strategies include Naïve Bayes (NB), Neural Network (NN), Linear Least Squares Fit (LLSF), k-Nearest-Neighbor (kNN), and Support Vector Machines (SVM); with SVMs showing better results in most cases. In this paper we introduce a new approach called dynamic architecture for artificial neural networks (DAN2) as an alternative for solving textual document classification problems. DAN2 is a scalable algorithm that does not require parameter settings or network architecture configuration. To show DAN2 as an effective and scalable alternative for text classification, we present comparative results for the Reuters-21578 benchmark dataset. Our results show DAN2 to perform very well against the current leading solutions (kNN and SVM) using established classification metrics.},
  doi      = {https://doi.org/10.1016/j.eswa.2012.03.027},
  keywords = {Classification, Textual document classification, Dynamic artificial neural networks, Pattern recognition, Machine learning, Artificial intelligence},
  url      = {http://www.sciencedirect.com/science/article/pii/S0957417412004976},
}

@InProceedings{Nam2014,
  author    = {Nam, Jinseok and Kim, Jungi and Loza Menc{\'i}a, Eneldo and Gurevych, Iryna and F{\"u}rnkranz, Johannes},
  title     = {Large-Scale Multi-label Text Classification --- Revisiting Neural Networks},
  booktitle = {Machine Learning and Knowledge Discovery in Databases},
  year      = {2014},
  editor    = {Calders, Toon and Esposito, Floriana and H{\"u}llermeier, Eyke and Meo, Rosa},
  pages     = {437--452},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Neural networks have recently been proposed for multi-label classification because they are able to capture and model label dependencies in the output layer. In this work, we investigate limitations of BP-MLL, a neural network (NN) architecture that aims at minimizing pairwise ranking error. Instead, we propose to use a comparably simple NN approach with recently proposed learning techniques for large-scale multi-label text classification tasks. In particular, we show that BP-MLL's ranking loss minimization can be efficiently and effectively replaced with the commonly used cross entropy error function, and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. Our experimental results show that simple NN models equipped with advanced techniques such as rectified linear units, dropout, and AdaGrad perform as well as or even outperform state-of-the-art approaches on six large-scale textual datasets with diverse characteristics.},
  isbn      = {978-3-662-44851-9},
}

@Article{Debole2005,
  author   = {Debole, Franca and Sebastiani, Fabrizio},
  title    = {An analysis of the relative hardness of Reuters-21578 subsets},
  journal  = {Journal of the American Society for Information Science and Technology},
  year     = {2005},
  volume   = {56},
  number   = {6},
  pages    = {584-596},
  abstract = {Abstract The existence, public availability, and widespread acceptance of a standard benchmark for a given information retrieval (IR) task are beneficial to research on this task, because they allow different researchers to experimentally compare their own systems by comparing the results they have obtained on this benchmark. The Reuters-21578 test collection, together with its earlier variants, has been such a standard benchmark for the text categorization (TC) task throughout the last 10 years. However, the benefits that this has brought about have somehow been limited by the fact that different researchers have “carved” different subsets out of this collection and tested their systems on one of these subsets only; systems that have been tested on different Reuters-21578 subsets are thus not readily comparable. In this article, we present a systematic, comparative experimental study of the three subsets of Reuters-21578 that have been most popular among TC researchers. The results we obtain allow us to determine the relative hardness of these subsets, thus establishing an indirect means for comparing TC systems that have, or will be, tested on these different subsets.},
  doi      = {10.1002/asi.20147},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.20147},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.20147},
}

@InProceedings{CapdevilaDalmau2007,
  author    = {Capdevila Dalmau, Marta and M{\'a}rquez Fl{\'o}rez, Oscar W.},
  title     = {Experimental Results of the Signal Processing Approach to Distributional Clustering of Terms on Reuters-21578 Collection},
  booktitle = {Advances in Information Retrieval},
  year      = {2007},
  editor    = {Amati, Giambattista and Carpineto, Claudio and Romano, Giovanni},
  pages     = {678--681},
  address   = {Berlin, Heidelberg},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {Distributional Clustering has showed to be an effective and powerful approach to supervised term extraction aimed at reducing the original indexing space dimensionality for Automatic Text Categorization [2]. In a recent paper [1] we introduced a new Signal Processing approach to Distributional Clustering which reached categorization results on 20 Newsgroups dataset similar to those obtained by other information-theoretic approaches [3][4][5] . Here we re-validate our method by showing that the 90-categories Reuters-21578 benchmark collection can be indexed with a minimum loss of categorization accuracy (around 2{\%} with Na{\"i}ve Bayes categorizer) with only 50 clusters.},
  isbn      = {978-3-540-71496-5},
}

@InProceedings{rose2002reuters,
  author       = {Rose, Tony and Stevenson, Mark and Whitehead, Miles},
  title        = {The Reuters Corpus Volume 1-from Yesterday's News to Tomorrow's Language Resources.},
  booktitle    = {Lrec},
  year         = {2002},
  volume       = {2},
  pages        = {827--832},
  organization = {Las Palmas},
}

@InProceedings{4403158,
  author    = {R. D. {Goyal}},
  title     = {Knowledge Based Neural Network for Text Classification},
  booktitle = {2007 IEEE International Conference on Granular Computing (GRC 2007)},
  year      = {2007},
  pages     = {542-542},
  month     = {Nov},
  doi       = {10.1109/GrC.2007.108},
  keywords  = {Bayes methods;neural nets;text analysis;knowledge based neural network;automatic text classification;Bayesian method;training documents;noise data;naive Bayesian text classification technique;Neural networks;Text categorization;Training data;Bayesian methods;Machine learning algorithms;Computer networks;Information technology;Learning systems;Data analysis;Probability distribution},
}

@InProceedings{7966144,
  author    = {G. {Chen} and D. {Ye} and Z. {Xing} and J. {Chen} and E. {Cambria}},
  title     = {Ensemble application of convolutional and recurrent neural networks for multi-label text categorization},
  booktitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
  year      = {2017},
  pages     = {2377-2383},
  month     = {May},
  doi       = {10.1109/IJCNN.2017.7966144},
  issn      = {2161-4407},
  keywords  = {computational complexity;feature extraction;feedforward neural nets;pattern classification;recurrent neural nets;text analysis;ensemble application;convolutional neural networks;recurrent neural networks;multilabel text categorization;text classification;document semantic information;multiclass text categorization;label combinations;local semantic information extraction;global textual semantics;local textual semantics;high-order label correlation modelling;tractable computational complexity;CNN-RNN model;large-sized dataset;Text categorization;Feature extraction;Semantics;Correlation;Logic gates;Predictive models;Computational modeling},
}

@Article{violos2018text,
  author  = {Violos, John and Tserpes, Konstantinos and Varlamis, Iraklis and Varvarigou, Theodora},
  title   = {Text Classification using the N-gram graph Representation model over high frequency data streams},
  journal = {Front. Appl. Math. Stat. 4: 41. doi: 10.3389/fams},
  year    = {2018},
}

@Article{Miao2009,
  author        = {Duoqian Miao and Qiguo Duan and Hongyun Zhang and Na Jiao},
  title         = {Rough set based hybrid algorithm for text classification},
  journal       = {Expert Systems with Applications},
  year          = {2009},
  volume        = {36},
  number        = {5},
  pages         = {9168 - 9174},
  issn          = {0957-4174},
  __markedentry = {[TIm:]},
  abstract      = {Automatic classification of text documents, one of essential techniques for Web mining, has always been a hot topic due to the explosive growth of digital documents available on-line. In text classification community, k-nearest neighbor (kNN) is a simple and yet effective classifier. However, as being a lazy learning method without premodelling, kNN has a high cost to classify new documents when training set is large. Rocchio algorithm is another well-known and widely used technique for text classification. One drawback of the Rocchio classifier is that it restricts the hypothesis space to the set of linear separable hyperplane regions. When the data does not fit its underlying assumption well, Rocchio classifier suffers. In this paper, a hybrid algorithm based on variable precision rough set is proposed to combine the strength of both kNN and Rocchio techniques and overcome their weaknesses. An experimental evaluation of different methods is carried out on two common text corpora, i.e., the Reuters-21578 collection and the 20-newsgroup collection. The experimental results indicate that the novel algorithm achieves significant performance improvement.},
  doi           = {https://doi.org/10.1016/j.eswa.2008.12.026},
  keywords      = {Text classification, Variable precision rough set (VPRS), -nearest neighbor (kNN), Rocchio algorithm},
  url           = {http://www.sciencedirect.com/science/article/pii/S0957417408008919},
}

@Article{DBLP:journals/corr/Kim14f,
  author        = {Yoon Kim},
  title         = {Convolutional Neural Networks for Sentence Classification},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1408.5882},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/Kim14f},
  eprint        = {1408.5882},
  timestamp     = {Mon, 13 Aug 2018 16:46:21 +0200},
  url           = {http://arxiv.org/abs/1408.5882},
}

@InCollection{Han2015,
  author        = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  title         = {Learning both Weights and Connections for Efficient Neural Network},
  booktitle     = {Advances in Neural Information Processing Systems 28},
  publisher     = {Curran Associates, Inc.},
  year          = {2015},
  editor        = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages         = {1135--1143},
  __markedentry = {[TIm:6]},
  url           = {http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf},
}

@Article{DBLP:journals/corr/abs-1901-08455,
  author        = {Yue Li and Weibin Zhao and Lin Shang},
  title         = {Really should we pruning after model be totally trained? Pruning based on a small amount of training},
  journal       = {CoRR},
  year          = {2019},
  volume        = {abs/1901.08455},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1901-08455},
  eprint        = {1901.08455},
  timestamp     = {Sat, 02 Feb 2019 16:56:00 +0100},
  url           = {http://arxiv.org/abs/1901.08455},
}

@Article{DBLP:journals/corr/abs-1808-08782,
  author        = {Alon Rozental and Daniel Fleischer and Zohar Kelrich},
  title         = {Amobee at {IEST} 2018: Transfer Learning from Language Models},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1808.08782},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1808-08782},
  eprint        = {1808.08782},
  timestamp     = {Sun, 02 Sep 2018 15:01:54 +0200},
  url           = {http://arxiv.org/abs/1808.08782},
}

@Article{wortsman2019discovering,
  author  = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  title   = {Discovering Neural Wirings},
  journal = {arXiv preprint arXiv:1906.00586},
  year    = {2019},
}

@Book{adamy2007fuzzy,
  title     = {Fuzzy Logik, Neuronale Netze und Evolution{\"a}re Algorithmen},
  publisher = {Shaker},
  year      = {2007},
  author    = {Adamy, J{\"u}rgen},
}

@Article{article,
  author  = {Poggio, Tomaso and Girosi, Federico},
  title   = {A Theory of Networks for Approximation and Learning},
  journal = {Tech Rep A.I. Memo No. 1140},
  year    = {2001},
  volume  = {1140},
  month   = {08},
}

@InProceedings{He_2015_ICCV,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  year      = {2015},
  month     = {December},
}

@Article{Russakovsky2015,
  author   = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
  title    = {ImageNet Large Scale Visual Recognition Challenge},
  journal  = {International Journal of Computer Vision},
  year     = {2015},
  volume   = {115},
  number   = {3},
  pages    = {211--252},
  month    = {Dec},
  issn     = {1573-1405},
  abstract = {The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5 years of the challenge, and propose future directions and improvements.},
  day      = {01},
  doi      = {10.1007/s11263-015-0816-y},
  url      = {https://doi.org/10.1007/s11263-015-0816-y},
}

@Article{lally2011natural,
  author  = {Lally, Adam and Fodor, Paul},
  title   = {Natural language processing with prolog in the ibm watson system},
  journal = {The Association for Logic Programming (ALP) Newsletter},
  year    = {2011},
}

@Article{silver2017mastering,
  author    = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  title     = {Mastering the game of go without human knowledge},
  journal   = {Nature},
  year      = {2017},
  volume    = {550},
  number    = {7676},
  pages     = {354},
  publisher = {Nature Publishing Group},
}

@Article{gibney2016google,
  author  = {Gibney, Elizabeth},
  title   = {Google AI algorithm masters ancient game of Go},
  journal = {Nature News},
  year    = {2016},
  volume  = {529},
  number  = {7587},
  pages   = {445},
}

@Article{ARON201124,
  author  = {Jacob Aron},
  title   = {How innovative is Apple's new voice assistant, Siri?},
  journal = {New Scientist},
  year    = {2011},
  volume  = {212},
  number  = {2836},
  pages   = {24},
  issn    = {0262-4079},
  doi     = {https://doi.org/10.1016/S0262-4079(11)62647-X},
  url     = {http://www.sciencedirect.com/science/article/pii/S026240791162647X},
}

@Article{Moor_2006,
  author       = {Moor, James},
  title        = {The Dartmouth College Artificial Intelligence Conference: The Next Fifty Years},
  journal      = {AI Magazine},
  year         = {2006},
  volume       = {27},
  number       = {4},
  pages        = {87},
  month        = {Dec.},
  abstractnote = {The Dartmouth College Artificial Intelligence Conference: The Next 50 Years (AI@50) took place July 13-15, 2006. The conference had three objectives: to celebrate the Dartmouth Summer Research Project, which occurred in 1956; to assess how far AI has progressed; and to project where AI is going or should be going. AI@50 was generously funded by the office of the Dean of Faculty and the office of the Provost at Dartmouth College, by DARPA, and by some private donors.},
  doi          = {10.1609/aimag.v27i4.1911},
  url          = {https://www.aaai.org/ojs/index.php/aimagazine/article/view/1911},
}

@Comment{jabref-meta: databaseType:bibtex;}
