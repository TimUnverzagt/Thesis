%*****************************************
\chapter{Related Work}
\label{ch:relatedwork}
%*****************************************
 The context of current research is needed to assess the utility of any results achieved during this thesis. As such, this section shortly presents the quality of state-of-the-art approaches that are covered in the following chapters.\footnote{The rankings presented in this chapter rely on the website: "Papers with Code".\\
 	https://paperswithcode.com/sota
 } Additionally, it gives an overview of previous pruning methods and their achievements. 

\section{State of the art: Image Classification}
\begin{figure}
	\begin{tabular}{c|c|c|c}
		Accuracy & MNIST & CIFAR-10 & Published\\
		\hline
		EnAET &  & 98.0 & 2019 \\
		DirNAS &  & 97.9 & 2019 \\
		Squee &  & 97.88 & 2019 \\
		RMDL & 99.82 & 91.2 & 2018 \\
		Simple & 99.8 & 95.5 & 2016 \\
		BatchNorm  & 99.8 & 93.3 & 2015 \\
		APAC & 99.8 & 89.7 & 2015\\
		Multi-Column & 99.8 & 88.8 & 2012 \\
		\hline
		Lenet-FCN & \textasciitilde98 &  & LTH \\
		Conv-6 &  & \textasciitilde80 & LTH \\
		
	\end{tabular}
	\caption{Performance for Image Classification}
\end{figure}
MNIST and CIFAR-10 are the two image datasets covered in this thesis, containing small images displaying grey-scale images of digits and colored images of real-world objects, respectively. Chapter \ref{ch:datasets} describes them in more detail.
The primary task on both of them demands a classification of their images into the ten corresponding classes. State of the art approaches delivers human or even superhuman accuracy on both data sets.\\
In their paper "RMDL: Random Multimodel Deep Learning" Kowsari et al. combined dense neural networks, convolutional neural networks, and recurrent neural networks into an ensemble, that achieves an accuracy of 99.82 percent for MNIST image classification as well as good benchmarks on other tasks, including CIFAR10 image classification and natural language processing tasks.\cite{RMDL}\\
The researchers of the next few approaches report accuracies of 99.8 percent, which equates to only two fewer misclassified images.\\
Already in 2012, D. Ciresan et al. described a deep architecture that alternates between convolution and softmax layers before it closes with a short dense classification structure, similar to the VGG16 architecture described in chapter \ref{ch:background}. At the time, it improved on the state-of-the-art of image classification over multiple datasets, including MNIST. They also recorded that their networks are structurally similar to the neural connection between the retina and the visual cortex of mammals such as macaque monkeys.\cite{Multi-Column}\\
Three years later, Later Sato et al. improved on approaches that use self-generated augmented data to increase the transformation invariance of their architecture in their paper "APAC: Augmented PAttern Classification with Neural Networks". They achieved the same results on MNIST image classification.\cite{APAC}\\
In the same year, C. Jia-Ren and C. Yong-Sheng published "Batch-normalized Maxout Network in Network", a paper in which they explain how they adopted the newly developed "Network in Network" structure to avoid vanishing gradients.\cite{Batch-Normalized}\\
 They also share second place alongside S. Hossein Hasanpour et alia.\cite{Keep-It-Simple}\\
\\
In contrast to MNIST, the three best-performing approaches for CIFAR-10 image classification were all published in the last two years. Currently, an automated reinforcement learning approach claims the highest performance of 98,3 percent.\footnote{Some researchers achieved better benchmarks but used additional data. Because most researchers, including J.Frankle and M. Carbin, did not use additional data on CIFAR10, we omitted those results.}
Through the work presented in their paper "Fast AutoAugment", S. Lim et al. improved the results of multiple existing architectures through automatic data enhancement. They remark that such automation was previously posed as an important open question because the efficiency of any single data enhancement strategy is highly dependant on the dataset at hand.\cite{Auto-Augment}\\
The second most accurate approach also utilizes data augmentation in addition to other techniques for the reduction of data dependency. Under the title "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", X. Wang et al. produced an approach very adept at handling tasks with little available data and very competitive when supplied by the same data as contemporary architectures. It achieves 98,01 percent accuracy.\cite{EnAET}\\
With their publication "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", Cai et al. supplied a competitive technique much more closely related to the content of this thesis than any of the previously mentioned. They addressed issues of computational expense in the previous network architecture search algorithm allowing them to increase the search space drastically. As a result, they find competitive networks capable of accuracies as high as 97.02 and smaller than those presented in previous state-of-the-art publications.\cite{Direct-NAS}\\ 
To complete the picture of performance measures figure [3.1] summarizes the state-of-the-art accuracies, as well the ones J. Frankle and M. Carbin, report for the two networks studied in this paper.   
While they do not provide exact values in the LTH-paper, their figures indicate that their Lenet-FCN and Conv-6 architectures achieve roughly 98 percent accuracy on MNIST and 80 percent on CIFAR-10, respectively. \cite{LTH} Additionally, it displays the best available estimates for human accuracy on both datasets. The authors of MNIST supplied an estimate in one of their papers, and T. Ho-Phuoc conducted a study on human proficiency for CIFAR10 image recognition.\cite{MNIST-Human}\cite{CIFAR10-Human}


\section{State of the art: Topic Classification}
\begin{figure}
	\begin{tabular}{c|c|c|c}
		Accuracy \% & 20-News & Reuters & Published\\
		\hline
		Neural BoE & 88.1 &  & 2019 \\
		Graph Star & 86.9 &  & 2019 \\
		RMDL &  & 90.69 & 2018 \\
		\hline
		multi-scale CNN & 86.12 &  & 2018 \\
		
	\end{tabular}
	\caption{Performance for Topic Classification}
\end{figure}
In the field of NLP topic classification is arguably the task most similar to image classification and Reuters-21578 is arguably the most iconic dataset for such a task. Yet neither do its corresponding state of the art architectures compare sensibly to the ones studied by Frankle \& Carbin nor is Reuters-21578 structurally akin to MNIST. The essential differences will be covered in section \ref{ch:data_sets}. \\
20-Newsgroup is another NLP data set not only more aligned with MNIST and CIFAR-10 but also with an competitive CNN architecture exists. In their work Pappagari et al. develop an approach integrating the implicit verification objective and learning multiple language models for different channels of their CNN \cite{End-to-End-CNN}. They come close to state of the art performance on 20-Newsgroup. 

\section{Pruning}
Beginning around 1990 with M. C. Mozer \& P. Smolensky \cite{Skeletonization} as well as LeCun et al. \cite{Optimal-Brain-Damage} weights were being removed from neural networks after training them for a task. Shortly thereafter the idea of further training a pruned network was proposed \cite{Optimal-Brain-Surgeon} which became common practice over the next decade. While LeCun et al. describe a network compression factor of $\times4$, more recent works achieve a factor of $\times9$ to $\times16.6$ while loosing no or close to no accuracy \cite{Learning_Weights_And_Connections} \cite{ThiNet}. Frankle \& Carbin report pruning rates over 98,5\% of weights in one of their networks while maintaining network capabilities which amounts to a compression rate of over $\times50$. \cite{LTH}
\\
In a recent paper \cite{Rethinking-Network-Pruning} Z.Liu et al. observe that if pruned networks are trained with randomly reinitialized weights instead of fine-tuning their previous ones they retain from the original network, the pruned networks keep their capabilities. They conclude that said weights can not be essential to a pruned networks quality, contrary to prior common belief. Thus Z.Liu et al. claim that the architecture of pruned networks is responsible for its capabilities and furthermore that pruning can be interpreted as a kind of network architecture search .\\
After the effectiveness of pruning is established and its interpretation as network architecture search becomes available there is a legitimate question whether all the weights in a network are really necessary for all of the training. In a paper of Y. Li \& W. Zhao \& L. Schang from early 2019 \cite{Pruning-With-Little-Training} they describe a method named IPLT to prune common convolutional network architectures at the filter level and especially before convergence. Thus they do not only compress the networks by a factor of $\times10$ but also speed up training by a similar magnitude. If the LTH can be applied in such a fashion a speed-up of up to $\times20$ should be expected.

\section{Additions to the Lottery Ticket Hypothesis}
Even though the Lottery-Ticket-Hypothesis was only proposed earlier this year additional papers on the topic exist.
In a paper from June 2019 J. Frankle \& M. Carbin et al. \cite{LTH-At-Scale} expand their method to find winning tickets on deep convolutional network architectures that proved difficult before. They attribute this achievement to the decision of not returning to the very first state of the network but to one a few iterations into training. Not only does this mark a lower limit for how early pruning is possible with the LTH but it also implies that a certain structure emerges after little training of the big network. Whether said structure only marks a point for valid reinitialization or rather already one for magnitude-based pruning is part of what this thesis wants to explore.\\
Additionally H. Zhou et al. \cite{Deconstructing_LTH} document an ablation study on the phenomenon of lottery tickets. They reaffirm the initially naive magnitude-based pruning and describe "supermasks" that improve accuracy when applied to the initial network even without additional training. Finally they find that a replacement of all weights in the pruned network by a constant with same sign as said weights does not significantly influence the networks capabilities. As such H. Zhou et al. conclude that the sign of weights are the essential property for such neural networks. 
