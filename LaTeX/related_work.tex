%*****************************************
\chapter{Related Work}
\label{ch:relatedwork}
%*****************************************
 The context of current research is needed to assess the utility of any results achieved during this thesis. As such, this section shortly presents the state-of-the-art of the relevant tasks and compares them to the approaches that are covered in the following chapters.\footnote{The rankings presented in this chapter rely primarily on the website: "Papers with Code".\\
 	https://paperswithcode.com/sota
 } Additionally, it gives an overview of previous pruning methods and their achievements. 

\section{State of the art: Image Classification}
MNIST and CIFAR-10 are the two image datasets covered in this thesis, containing small images displaying grey-scale images of digits and colored images of real-world objects, respectively. Chapter \ref{ch:datasets} describes them in more detail.
The primary task on both of them demands a classification of their images into the ten corresponding classes. State of the art approaches delivers human or even superhuman accuracy on both data sets.\\
In their paper "RMDL: Random Multimodel Deep Learning" Kowsari et al. combined dense neural networks, convolutional neural networks, and recurrent neural networks into an ensemble, that achieves an accuracy of 99.82 percent for MNIST image classification as well as good benchmarks on other tasks, including CIFAR10 image classification and natural language processing tasks.\cite{RMDL}\\
The researchers of the next few approaches report accuracies of 99.8 percent, which equates to only two fewer correctly classified images.\\
Back in 2012, D. Ciresan et al. described a deep architecture that alternates between convolution and softmax layers before it closes with a short dense classification structure, similar to the VGG16 architecture described in chapter \ref{ch:background}. Their paper "Multi-column Deep Neural Networks for Image Classification" improved on the state-of-the-art of image classification over multiple datasets, including MNIST. They also recorded that their networks are structurally similar to the neural connection between the retina and the visual cortex of mammals such as macaque monkeys.\cite{Multi-Column}\\
Three years later, Later Sato et al. polished approaches that use self-generated augmented data to increase the transformation invariance of their architecture in their paper "APAC: Augmented PAttern Classification with Neural Networks". They achieved the same results on MNIST image classification.\cite{APAC}\\
In the same year, C. Jia-Ren and C. Yong-Sheng published "Batch-normalized Maxout Network in Network", a paper in which they explain how they adopted the newly developed "Network in Network" structure to avoid vanishing gradients.\cite{Batch-Normalized}\\
 They also share second place alongside S. Hossein Hasanpour et alia.\cite{Keep-It-Simple}\\
\\
\newpage
In contrast to MNIST, the three best-performing approaches for CIFAR-10 image classification were all published in the last year. Currently, an automated reinforcement learning approach claims the highest performance of 98,3 percent.\footnote{Some researchers achieved better benchmarks but used additional data. Because most researchers, including J.Frankle and M. Carbin, did not use additional data on CIFAR10, we omitted those results.}
In their paper "Fast AutoAugment", S. Lim et al. improved the result of multiple existing architectures through automatic data enhancement. They remark that such automation was previously posed as an important open question because the efficiency of any single data enhancement strategy is highly dependant on the dataset at hand.\cite{Auto-Augment}\\
The second most accurate approach also utilizes data augmentation in addition to other techniques for the reduction of data dependency. Under the title "EnAET: Self-Trained Ensemble AutoEncoding Transformations for Semi-Supervised Learning", X. Wang et al. produced an approach which is very adept at handling tasks with little available data and very competitive when supplied by the same data as contemporary architectures. It achieves 98,01 percent accuracy.\cite{EnAET}\\
With their publication "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", Cai et al. supplied a competitive technique much more closely related to the content of this thesis than any of the previously mentioned. They addressed issues of computational expense in the previous network architecture search algorithm allowing them to increase the search space drastically. As a result, they find competitive networks capable of accuracies as high as 97.02 and smaller than those presented in previous state-of-the-art publications.\cite{Direct-NAS}\\
\\ 
To complete the picture of performance measures the following table summarizes the state-of-the-art accuracies, as well the ones J. Frankle and M. Carbin, report for the two networks studied in this paper.   
While they do not provide exact values in the Lottery Ticket Hypothesis paper, their figures indicate that their Lenet-FCN and Conv-6 architectures achieve roughly 98 percent accuracy on MNIST and 80 percent on CIFAR-10, respectively. \cite{LTH} Additionally, it displays the best available estimates for human accuracy on both datasets. The authors of MNIST supplied an estimate in one of their papers, and T. Ho-Phuoc conducted a study on human proficiency for CIFAR10 image recognition.\cite{MNIST-Human}\cite{CIFAR10-Human}\\
The architectures studied by J. Frankle and M. Carbin fall behind any state-of-the-art approaches by a significant margin, but they noted that they primarily studied smaller architectures due to the computational expense of their approach.\cite{LTH}
We support the validity of their experiments, as their results and conjectures are based solely on the relative performance of the networks they pruned compared to their complete counterpart.\\
\begin{tabularx}{\textwidth}{X X X X X}
	\multicolumn{5}{c}{\textbf{Contemporary performance for image classification.}}\\
	%\\
	\hline
	\endhead
	\\
	%[-2pt]
	\textbf{MNIST} & RMDL & 99.82 & 2018 & \cite{RMDL}\\
	%[8pt]
	& Human & \textasciitilde99.8 & -- & \cite{MNIST-Human}\\
	%[8pt]
	& Multi-Column & 99.8 & 2012 & \cite{Multi-Column}\\
	%[8pt]
	& APAC & 99.8 & 2015 & \cite{APAC}\\
	%[8pt]
	& Maxout Network & 99.8 & 2015 & \cite{Batch-Normalized}\\
	%[8pt]
	& LTH Lenet FCN & \textasciitilde98 & 2019 & \cite{LTH}\\
	[14pt]
	\textbf{CIFAR10} & AutoAugment & 98.3 & 2019 & \cite{Auto-Augment}\\
	%[8pt]
	& EnAET & 98.0 & 2019 & \cite{EnAET}\\
	%[8pt]
	& Direct NAS & 97.88 & 2019 & \cite{Direct-NAS}\\
	%[8pt]
	& Human & \textasciitilde94 & -- & \cite{CIFAR10-Human}\\
	%[8pt]
	& Maxout Network & 93.3 & 2015 & \cite{Batch-Normalized}\\
	%[8pt]
	& RMDL & 91.2 & 2018 & \cite{RMDL}\\
	%[8pt]
	& Multi-Column & 88.8 & 2018 & \cite{Multi-Column}\\
	%[8pt]
	& LTH Conv-6 & \textasciitilde80 & 2019 & \cite{LTH}\\
	%[8pt]
	[10pt]
	\hline
\end{tabularx}


%-----

\section{State of the art: Topic Classification}
Document topic classification is arguably the task most similar to image classification in the field of natural language processing. While Reuters-
21578 is arguably the most iconic dataset for this task, the architectures commonly applied to it are not related to the ones studied by J. Frankle and M. Carbin, nor is Reuters-21578 structurally
akin to MNIST. Section \ref{ch:datasets} covers a few differentiating properties.
20-Newsgroups is another natural language data set utilized for document classification which soime contemporary researches approached with architectures primarily composed of dense and convolutional layers instead of recurrent networks more common in the field of natural language processing.\\
In their work Pappagari et al. develop an approach that integrates multiple self-learned word-level language embeddings into a simple ensemble.\cite{End-to-End-CNN}
While newer architectures form the current state-of-the-art, their work achieved the best performance of its time on 20-Newsgroups. The following table summarizes the available measures for document topic classification on 20Newsgroups.

\begin{tabularx}{\textwidth}{X X X X X}
	\multicolumn{5}{c}{\textbf{Contemporary performance for document topic classification.}}\\
	%\\
	\hline
	\endhead
	\\
	\textbf{20Newsgroups} & Neural Bag of Entities & 88.1 & 2019 & \cite{Neural-BoE}\\
	%[8pt]
	& RMDL & 87.91 & 2018 & \cite{RMDL}\\
	%[8pt]
	& Graph Star & 86.9 & 2018 & \cite{Graph-Star}\\
	%[8pt]
	& End to End CNN & 86.12 & 2018 & \cite{End-to-End-CNN}\\
	%[8pt]
	& RMDL & 87.91 & 2018 & \cite{RMDL}\\
	%[8pt]
	[10pt]
	\hline
\end{tabularx}

%----

\section{Pruning}
Beginning around 1990 with M. C. Mozer and P. Smolensky as well as LeCun et al., researchers removed weights from neural networks after training them for a task.\cite{Skeletonization}\cite{Optimal-Brain-Damage}\\  Shortly after, B. Hassibi and D. G. Stork proposed further training of a pruned network \cite{Optimal-Brain-Surgeon}, which became standard practice over the next decade. While LeCun et al. reported that they compressed a network by the factor of four, more recent works achieve a factor of nine to about seventeen while losing little or no accuracy.\cite{Learning_Weights_And_Connections}\cite{ThiNet}\\
J. Frankle and M. Carbin reported pruning over 98,5\% of weights in one of their networks while maintaining network capabilities, which amounts to a compression of over 50 times.\cite{LTH}\\
\\
In their recent paper "Rethinking the value of network pruning", Z. Liu et al. observed that pruned networks display the same capabilities, whether they retrained them with randomly reinitialized weights or retain and fine-tune the previous weights. They concluded that said weights can not be essential to a pruned network's quality, contrary to prior common belief. Thus Z. Liu et al. claimed that the architecture of pruned networks is responsible for its capabilities and that pruning can be interpreted as a kind of network architecture search.\cite{Rethinking-Network-Pruning}\\
Based on the conclusion that trained weights only inform a search on the original architecture, it seems natural to question the amount of training necessary for them to establish their information content.\\
In a paper of Y. Li et el. from early 2019, they described a method named "Incremental pruning based on less training" for pruning of common convolutional network architectures at the filter level and especially before convergence. They compressed the networks by a factor of ten but also sped up training by a similar margin.\cite{Pruning-With-Little-Training}

\section{Additions to the Lottery Ticket Hypothesis}
Even though J. Frankle and M. Carbin only proposed the Lottery-Ticket-Hypothesis early in 2019, additional papers on the topic exist. In a follow-up-paper from June 2019, and with the help of a few additional authors, they expanded their method to find winning tickets on deep convolutional network architectures that proved difficult before. They attributed this achievement to the decision of not returning to the very first state of the network but to one a few iterations into training.\cite{LTH-At-Scale}\\
H. Zhou et al. recently performed an ablation study on the phenomenon of lottery tickets, which they documented in their paper "Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask".
They reaffirmed magnitude-based pruning and described super masks that improve accuracy when applied to the initial network even without additional training. Subsequently, they concluded that the masking of weights acts as training, moving weights in a direction that actual training would have taken them. Additionally, H. Zhou et al. found that a replacement of all weights in the pruned network by a constant with the same sign does not significantly influence the network's capabilities. They concluded that the sign of weights is the essential property for such neural networks.\cite{Deconstructing_LTH}
