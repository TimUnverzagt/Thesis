%*****************************************
\chapter{Related Work}
\label{ch:relatedwork}
%*****************************************
To quantify the goals previously defined the context of current research is needed. The importance of any work assuming an underlying architecture can not be correctly evaluated without knowledge about the quality of said architecture. As such this section shortly presents state-of-the-art approaches to the tasks relevant to this thesis. Additionally an overview over previous compression methods and their achievements is given.


\section{State of the art: Image Classification}
\begin{figure}
	\begin{tabular}{c|c|c|c}
		Accuracy \% & MNIST & CIFAR-10 & Published\\
		\hline
		EnAET &  & 98.0 & 2019 \\
		DirNAS &  & 97.9 & 2019 \\
		Squee &  & 97.88 & 2019 \\
		RMDL & 99.82 & 91.2 & 2018 \\
		Simple & 99.8 & 95.5 & 2016 \\
		BatchNorm  & 99.8 & 93.3 & 2015 \\
		APAC & 99.8 & 89.7 & 2015\\
		Multi-Column & 99.8 & 88.8 & 2012 \\
		\hline
		Lenet-FCN & \textasciitilde98 &  & LTH \\
		VGG-19 &  & \textasciitilde90 & LTH \\
		
	\end{tabular}
	\caption{Performance for Image Classification}
\end{figure}
MNIST and CIFAR-10 are both datasets containing small images which are to be classified according to the object they display. While MNIST contains gray scale images of hand-written digits CIFAR-10 consists of colorful real-world images. State of the art approaches deliver superhuman accuracy on both data sets.\\
For MNIST Kowasari et al. with their random multi-purpose deep learning ensemble report the nominal highest performance \cite{RMDL} although many others achieve similar results through varying means.\\
Already in 2012 Ciresan et al. describe a deep and sparse convolutional architecture that resembles the visual cortex of mammal \cite{Multi-Column}. Later Sato et al. apply data-augmentation \cite{APAC}, Chang Jia-Ren \& Chen Yong-Sheng package whole architectures and treat them like layers \cite{Batch-Normalized} and Hasanpour et al. carefully design a small and simple convolutional network through the use of structural heuristics \cite{Keep-It-Simple} all reproducing the same performance.\\
In contrast the three best-performing approaches to CIFAR-10 are all published in 2019. Currently an ensemble of auto-encoding transformations claims the highest performance. Wang et al. provide their model with a rich class of transformations to prepare abstraction of the input. \cite{EnAET}. Close second and third are Cai et al. with a direct network-architecture-search scheme \cite{Direct-NAS} and Hu et al. with a novel network building block that explicitly models interaction between channels \cite{Squee}. \\ 
While Frankle \& Carbin do not provide exact values in the LTH-paper their figures indicate that they achieve roughly 98\% accuracy on MNIST and 90\% on CIFAR-10 \cite{LTH}.\footnote
{State-of-the-Art architectures are presented only if no extra training data was used and as described on \href{https://paperswithcode.com/sota}{https://paperswithcode.com/sota}} This result is reproducible with the source code provided alongside this thesis.


\section{State of the art: Topic Classification}
\begin{figure}
	\begin{tabular}{c|c|c|c}
		Accuracy \% & 20-News & Reuters & Published\\
		\hline
		Neural BoE & 88.1 &  & 2019 \\
		Graph Star & 86.9 &  & 2019 \\
		RMDL &  & 90.69 & 2018 \\
		\hline
		multi-scale CNN & 86.12 &  & 2018 \\
		
	\end{tabular}
	\caption{Performance for Topic Classification}
\end{figure}
In the field of NLP topic classification is arguably the task most similar to image classification and Reuters-21578 is arguably the most iconic dataset for such a task. Yet neither do its corresponding state of the art architectures compare sensibly to the ones studied by Frankle \& Carbin nor is Reuters-21578 structurally akin to MNIST. The essential differences will be covered in section \ref{ch:data_sets}. \\
20-Newsgroup is another NLP data set not only more aligned with MNIST and CIFAR-10 but also with an competitive CNN architecture exists. In their work Pappagari et al. develop an approach integrating the implicit verification objective and learning multiple language models for different channels of their CNN \cite{End-to-End-CNN}. They come close to state of the art performance on 20-Newsgroup. 

\section{Pruning}
Beginning around 1990 with M.C. Mozer \& P. Smolensky \cite{Skeletonization} as well as LeCun et al. \cite{Optimal-Brain-Damage} weights were being removed from neural networks after training them for a task. Shortly thereafter the idea of further training a pruned network was proposed \cite{Optimal-Brain-Surgeon} which became common practice over the next decade. While LeCun et al. describe a network compression factor of $\times4$, more recent works achieve a factor of $\times9$ to $\times16.6$ while loosing no or close to no accuracy \cite{Learning_Weights_And_Connections} \cite{ThiNet}. Frankle \& Carbin report pruning rates over 98,5\% of weights in one of their networks while maintaining network capabilities which amounts to a compression rate of over. \cite{LTH} $\times50$

In a recent paper \cite{Rethinking-Network-Pruning} Z.Liu et al. observe that if pruned networks are trained with randomly reinitialized weights instead of fine-tuning their previous ones they retain from the original network, the pruned networks keep their capabilities. They conclude that said weights can not be essential to a pruned networks quality, contrary to prior common belief. Thus Z.Liu et al. claim that the architecture of pruned networks is responsible for its capabilities and furthermore that pruning can be interpreted as a kind of network architecture search .\\
After the effectiveness of pruning is established and its interpretation as network architecture search becomes available there is a legitimate question whether all the weights in a network are really necessary for all of the training. In a paper of Y. Li \& W. Zhao \& L. Schang from early 2019 \cite{Pruning-With-Little-Training} they describe a method named IPLT to prune common convolutional network architectures at the filter level and especially before convergence. Thus they do not only compress the networks by a factor of $\times10$ but also speed up training by a similar magnitude. If the LTH can be applied in such a fashion a speed-up of up to $\times20$ should be expected.

\section{Additions to the Lottery Ticket Hypothesis}
Even though the Lottery-Ticket-Hypothesis was only proposed earlier this year additional papers on the topic exist.
In a paper from June 2019 J. Frankle \& M. Carbin et al. \cite{LTH-At-Scale} expand their method to find winning tickets on deep convolutional network architectures that proved difficult before. They attribute this achievement to the decision of not returning to the very first state of the network but to one a few iterations into training. Not only does this mark a lower limit for how early pruning is possible with the LTH but i also implies that a certain structure emerges after little training of the big network. Whether said structure only marks a point for valid reinitialization or rather already one for magnitude-based pruning is part of what this thesis wants to explore.\\
Additionally H. Zhou et al. \cite{Deconstructing_LTH} document an ablation study on the phenomenon of lottery tickets. They reaffirm the initially naive magnitude-based pruning and describe "supermasks" that improve accuracy when applied to the initial network even without additional training. Finally they find that a replacement of all weights in the pruned network by a constant with same sign as said weights does not significantly influence the networks capabilities. As such H. Zhou et al. conclude that the sign of weights are the essential property for such neural networks. 
