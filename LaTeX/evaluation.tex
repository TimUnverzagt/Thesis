%*****************************************
\chapter{Evaluation}
\label{ch:evaluation}
%*****************************************
%\hint{This chapter should describe how the evaluation of the implemented mechanism was done. \\ \\
%1. Which evaluation method is used and why? Simulations, prototype? \\
%2. What is the goal of the evaluation? Comparison? Proof of concept? \\
%3. Wich metrics are used for characterizing the performance, costs, fairness, and efficiency of the system?\\
%4. What are the parameter settings used in the evaluation and why? If possible always justify why a certain threshold has been chose for a particular parameter.  \\
%5. What is the outcome of the evaluation? \\ \\
%The section should have a length of about five to ten pages.}
The following chapter presents the results achieved throughout this thesis and aims to explain their evaluation. First, the intended goal of each experiment is stated, including a  formulation in terms of validatable benchmarks. Subsequently, the process which extracts legible data from the framework is developed. The visualized data follows, and finally, the results are analyzed.

\section{Reproduction}
\subsection*{Goal and Benchmarks}
As the latter sections evaluate explorative experiments, the validity of the framework which supports them is crucial. We trained the first two models described in sections \ref{ch:design} and \ref{ch:implementation} under the conditions J. Frankle and M. Corbin describe in their paper. The previously described mismatch of weights forms the only known difference.
J. Frankle and M. Corbin primarily report the accuracy achieved by their implementations at the epoch of a simple stopping criterion. They executed each experiment five times and plot the mean alongside the minimum and maximum, which are represented through error bars. Additionally, they reperformed the same experiments ten times, but applied the masks, found after each epoch, to randomly reinitialized networks. The results were visualized in the same manner. \cite{LTH}
Figure \ref{fig:MNIST-Lenet-FCN-LTH} and figure \ref{fig:CIFAR10-Conv6-LTH} present these measures for the MNIST-FCN and CIFAR10-CNN-6, respectively. Both plots are taken from the Lottery Ticket Hypothesis paper, but we cleaned up the latter one and brought it up to scale for improved legibility.\footnote{The original figure is available in the paper of J. Frankle and M. Carbin. \cite{LTH}}  The goal of this reproduction is to produce results within the reported confidence intervals of accuracy in the Lottery Ticket Hypothesis paper.  

\subsection*{Evaluation Setup}
As our framework does not provide any early stopping criterion, we display the full range of accuracy a given architecture achieves after convergence. A line visualizes the mean, while a gray band denotes the interval between maximal and minimal achieved accuracy. As a positive side-effect, this setup increases the breadth of visualized data in the following explorative experiments because it is parameter-agnostic concerning the early stopping criterion. Finally,  as J. Frankle and M. Corbin present training accuracies for the MNIST-Lenet-FCN architecture, we visualize the same measure as additional points of comparison.\footnote{For the Conv6 architecture, the effective pruning rate per iteration amalgamates its dense and convolutional pruning rate. The disagreement in the number of weights in the dense layers between our framework and the Lottery Ticket Hypothesis paper also results in a different pruning rate per epoch. Furthermore J. Frankle and M. Carbin arrive at slightly different pruning rates after rounding.\\ While the labels nominally differ, we visualized our data with a grid corresponding to the same amount of training or pruning. It reduces legibility slightly in the case of Conv6 but increases ease of comparison across the board.
} 
\subsection*{Evaluation Results}
As seen in figures \ref{fig:MNIST-Lenet-FCN-LTH} and \ref{fig:MNIST-Lenet-FCN-Thesis}, the MNIST-Lenet-FCN implementation we provide achieves a lower accuracy than reported by J. Frankle and M. Corbin. Additionally, it does not show an interim improvement and degrades significantly faster at advanced pruning iterations. Said degradation is qualitatively similar to the behavior of the randomly reinitialized networks.
The comparison between figure \ref{fig:CIFAR10-Conv6-LTH} and figure \ref{fig:CIFAR10-Conv6-Thesis} yields similar results. While the CIFAR10-CNN-6 implementation produces the same accuracy as a full network, it degrades as quickly as J. Frankel and M. Corbin's reinitialized networks. Most of its graph falls into the deviation intervals they visualized.

\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_3_legend.png}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/7-Evaluation/LTH_0.png}
		\caption{LTH: MNIST-Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			"The Lottery Ticket Hypothesis" \cite{LTH}
		}
		\label{fig:MNIST-Lenet-FCN-LTH}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Reproduction-MNIST-FCN/accuracy/converged.png}
		\caption{Thes: MNIST-Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:MNIST-Lenet-FCN-Thesis}
	\end{minipage}
\end{figure}

\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_4_legend.png}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/7-Evaluation/LTH_CNN_clean.png}
		\caption{LTH: CIFAR10-Conv6}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			"The Lottery Ticket Hypothesis" \cite{LTH}
		}
		\label{fig:CIFAR10-Conv6-LTH}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Reproduction-CIFAR10-CNN/accuracy/LTH.png}
		\caption{Thesis: CIFAR10-Conv6}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:CIFAR10-Conv6-Thesis}
	\end{minipage}
\end{figure}

\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_1_legend.png}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/7-Evaluation/LTH_1.png}
		\caption{LTH: Slightly pruned Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			"The Lottery Ticket Hypothesis" \cite{LTH}
		}
		\label{fig:Slightly-Pruned-Lenet-LTH}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Reproduction-MNIST-FCN/accuracy/LTH_1.png}
		\caption{Thesis: Slightly pruned Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Slightly-Pruned-Lenet-Thesis}
	\end{minipage}
\end{figure}

\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_2_legend.png}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/7-Evaluation/LTH_2.png}
		\caption{LTH: Heavily pruned Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			"The Lottery Ticket Hypothesis" \cite{LTH}
		}
		\label{fig:Heavily-Pruned-Lenet-LTH}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Reproduction-MNIST-FCN/accuracy/LTH_2.png}
		\caption{Thesis: Heavily pruned Lenet-FCN}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Heavily-Pruned-Lenet-Thesis}
	\end{minipage}
\end{figure}

\subsection*{Analysis of Results}
The implementations of the provided framework do not reproduce the results presented in the Lottery Ticket Hypothesis paper, meaning that the framework is not validated. This poses the question of how to proceed with the remaining experiments, which we discuss in the corresponding "Goal and Benchmarks" subsections.
While the framework did not fulfill the primary goal of the experiment, two phenomena remain unclear and intriguing:
The MNIST-Lenet-FCN architecture produced through our implementation of the Lottery Ticket algorithm degrades only by one percentage point under compression of up to ten times. Such a result is comparable to various pruning methods discussed in chapter \ref{ch:relatedwork}. 

%*****************************************

\section{Transfer}
\subsection*{Goal and Benchmarks}
While the reproduction experiments did not validate the framework, it still pruned the MNIST-Lenet-FCN to a degree comparable to contemporary work and it did so through the masking of networks with frozen initialization. As such, the transfer to another field still might yield a new pruning tool for additional tasks and applications. If the framework prunes about 90 percent of weights without the sacrifice of prediction quality, we consider the transfer successful.
\subsection*{Evaluation Setup}
Figure \ref{fig:20Newsgroups-Converged} presents benchmarks collected in the same manner as for the other architectures, but fewer pruning iterations are displayed. Additionally, figure \ref{fig:20Newsgroups-Training} provides the training accuracy we used to determine the amount of training necessary for convergence. The sheer size of the 20Newsgroups-End2End architecture severely limited the number of pruning iterations, a single experiment could afford to run without overflowing the memory of our computing devices. We managed to perform experiments with ten pruning iterations, the minimal number to prune a network to a competitive degree. 
\subsection*{Evaluation Results}
The right side of figure 7.4 shows that the 20Newsgroups-End2End architecture retains its accuracy even if about 90 percent of its weights are pruned. Additionally, the networks of the intermediate pruning epochs show an accuracy improvement of at least one percentage point.
\begin{figure}
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/20Newsgroups_legend.png}
	\end{minipage}
	\begin{minipage}{0.5\textwidth}
		\centering
		%\includegraphics[width=100px]{gfx/7-Evaluation/LTH_4_legend.png}
	\end{minipage}
	\\
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Transfer-20Newsgroups-CNN/accuracy/10_iterations.png}
		\caption{Training history on 20Newsgroups}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:20Newsgroups-Training}
	\end{minipage}\hfill
	\begin{minipage}{0.5\textwidth}
		\centering
		\includegraphics[height=180px]{gfx/Experiments/Transfer-20Newsgroups-CNN/accuracy/converged.png}
		\caption{Accuracy after convergence}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:20Newsgroups-Converged}
	\end{minipage}
\end{figure}

\subsection*{Analysis of Results}
According to the previously defined goal, the transfer was successful, which shows that non-image-recognition architectures can contain efficient subnetworks upon initialization.

%*****************************************
\newpage

\section{Early Tickets}
Independent of the ability to recover actual lottery tickets, the pruning implementation supplied by our framework finds small subnetworks in the initialization which are trainable to a nontrivial accuracy. H. Zhou et al. findings confirm that the utilization of trained weights, to calculate the pruning mask, is essential.\cite{Deconstructing_LTH} Information on the development of the quality of said weights is still of interest. 
\subsection*{Goal and Benchmarks}
The aim of this experiment is purely explorative. If any patterns are recognized, they may inform experiments implemented with valid frameworks.
\subsection*{Evaluation Setup}
Figures \ref{fig:Early-Tickets-0} through \ref{fig:Early-Tickets-9} plot the mean accuracy achieved by implementations set to prune at the n-th epoch of training. To avoid visual clutter, the intervals representing minimal and maximal values are omitted. 
\subsection*{Evaluation Results}
Because the accuracy of multiple graphs behaves erratically, it is challenging to discern a development over the training depth. The only certain observation is that all trials of earlier pruning degrade significantly faster than the original approach.
\subsection*{Analysis of Results}
While the original network converges in about ten epochs, the same amount of training does not seem to suffice to achieve the full efficiency of the pruning algorithm implemented in our framework

\newpage
\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_5_legend.png}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[height=175px]{gfx/Experiments/EarlyTicket-MNIST-FCN/012.png}
		\caption{Networks pruned at epochs 0|1|2}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Early-Tickets-0}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[height=175px]{gfx/Experiments/EarlyTicket-MNIST-FCN/345.png}
		\caption{Networks pruned at epochs 3|4|5}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Early-Tickets-4}
	\end{minipage}
\end{figure}
\begin{figure}
	\begin{minipage}{\textwidth}
		\centering
		\includegraphics[width=100px]{gfx/7-Evaluation/LTH_6_legend.png}
	\end{minipage}
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[height=175px]{gfx/Experiments/EarlyTicket-MNIST-FCN/678.png}
		\caption{Networks pruned at epochs 6|7|8}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Early-Tickets-6}
	\end{minipage}\hfill
	\begin{minipage}{0.45\textwidth}
		\centering
		\includegraphics[height=175px]{gfx/Experiments/EarlyTicket-MNIST-FCN/910.png}
		\caption{Networks pruned at epochs 9 and 10}
		\vspace{7pt}
		\footnotesize{
			Source:\\
			This graph was produced by the author.
		}
		\label{fig:Early-Tickets-9}
	\end{minipage}
\end{figure}
